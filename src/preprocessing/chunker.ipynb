{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763040402433,
     "user_tz": -180,
     "elapsed": 9982,
     "user": {
      "displayName": "Alexander Belyanchikov",
      "userId": "09336036699414271733"
     }
    },
    "outputId": "24e495e3-217a-483c-8f1d-da4eb930f647"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n"
     ]
    }
   ],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "!pip install pandas openpyxl sentence-transformers scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# –í—Å—Ç–∞–≤–ª—è–µ–º –≤–µ—Å—å –∫–æ–¥ chunker.py —Å—é–¥–∞\n",
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è ---\n",
    "def setup_logger(output_dir: str):\n",
    "    log_path = os.path.join(output_dir, \"chunker.log\")\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path, mode=\"w\", encoding=\"utf-8\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(\"chunker\")\n",
    "\n",
    "\n",
    "# --- –ó–∞–≥—Ä—É–∑–∫–∞ NOISE_PATTERNS –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ) ---\n",
    "def load_noise_patterns(config_path: str = \"noise_patterns.json\"):\n",
    "    default_patterns = [\n",
    "        r'\\?oirutpspid=[^&\\s]*',\n",
    "        r'\\?oirutpspsc=[^&\\s]*',\n",
    "        r'\\?oirutpspjs=[^&\\s]*',\n",
    "        r'#\\S*',\n",
    "        r'tel\\.', r'Tel\\.', r'—Ç–µ–ª\\.', r'–¢–µ–ª\\.', r'—Ç–µ–ª:', r'–¢–µ–ª:',\n",
    "        r'¬©\\s*–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫',\n",
    "        r'–ü–æ–ª—å–∑—É—è—Å—å —Å–∞–π—Ç–æ–º',\n",
    "        r'—Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏',\n",
    "        r'–ö–æ–Ω—Ç–∞–∫—Ç—ã', r'–ü–æ–¥–¥–µ—Ä–∂–∫–∞', r'–ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞',\n",
    "        r'–ú–æ—Å–∫–≤–∞', r'–†–æ—Å—Å–∏—è',\n",
    "        r'199\\d‚Äì202\\d',\n",
    "        r'https?://[^\\s]+\\.(?:png|jpg|jpeg|gif|pdf)',\n",
    "        r'\\{[^{}]*\\}', # JSON-—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã\n",
    "    ]\n",
    "\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                config = json.load(f)\n",
    "                return config.get(\"noise_patterns\", default_patterns)\n",
    "            except Exception:\n",
    "                print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞ {config_path}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\")\n",
    "                return default_patterns\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥ {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\")\n",
    "        return default_patterns\n",
    "\n",
    "\n",
    "NOISE_PATTERNS = load_noise_patterns()\n",
    "\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"–£–¥–∞–ª–µ–Ω–∏–µ HTML –∏ —à—É–º–∞\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    for pattern in NOISE_PATTERNS:\n",
    "        s = re.sub(pattern, '', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'<script.*?</script>', ' ', s, flags=re.IGNORECASE | re.DOTALL)\n",
    "    s = re.sub(r'<style.*?</style>', ' ', s, flags=re.IGNORECASE | re.DOTALL)\n",
    "    s = re.sub(r'<[^>]+>', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = re.sub(r'([.,;!?])\\1+', r'\\1', s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def semantic_chunk_by_similarity(text: str, model, max_chunk_len: int = 400, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —á–∞–Ω–∫–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ—Å–µ–¥–Ω–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "    \"\"\"\n",
    "    sentences = [s.strip() for s in re.split(r'\\n\\s*\\n|\\. ', text) if s.strip()]\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    if len(sentences) < 2:\n",
    "        # –ü—Ä–æ—Å—Ç–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –¥–ª–∏–Ω–µ\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        for sent in sentences:\n",
    "            if len(current_chunk) + len(sent) < max_chunk_len:\n",
    "                current_chunk += \" \" + sent\n",
    "            else:\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sent\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(current_chunk.strip())\n",
    "        return [c for c in chunks if len(c) >= 100]\n",
    "\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = sentences[0]\n",
    "    current_emb = embeddings[0].reshape(1, -1)\n",
    "\n",
    "    for i in range(1, len(sentences)):\n",
    "        next_emb = embeddings[i].reshape(1, -1)\n",
    "        similarity = cosine_similarity(current_emb, next_emb)[0][0]\n",
    "\n",
    "        if similarity > threshold and len(current_chunk) + len(sentences[i]) < max_chunk_len:\n",
    "            current_chunk += \" \" + sentences[i]\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentences[i]\n",
    "            current_emb = next_emb\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return [c for c in chunks if len(c) >= 100]\n",
    "\n",
    "\n",
    "def hash_text(text: str) -> str:\n",
    "    \"\"\"–•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏\"\"\"\n",
    "    return hashlib.md5(text.encode('utf-8', errors='ignore')).hexdigest()\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    output_dir = os.path.dirname(args.output)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    logger = setup_logger(os.path.dirname(args.output))\n",
    "\n",
    "    logger.info(f\"–ß—Ç–µ–Ω–∏–µ {args.input}...\")\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
    "    if args.input.lower().endswith('.xlsx'):\n",
    "        try:\n",
    "            df = pd.read_excel(args.input, engine=\"openpyxl\")\n",
    "        except ImportError:\n",
    "            raise ImportError(\"–î–ª—è —á—Ç–µ–Ω–∏—è .xlsx –Ω—É–∂–µ–Ω –ø–∞–∫–µ—Ç 'openpyxl'. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–≥–æ: pip install openpyxl\")\n",
    "    elif args.input.lower().endswith('.csv'):\n",
    "        df = pd.read_csv(args.input)\n",
    "    else:\n",
    "        raise ValueError(f\"–§–∞–π–ª {args.input} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è .csv –∏–ª–∏ .xlsx\")\n",
    "\n",
    "    required_cols = {\"web_id\", \"url\", \"kind\", \"title\", \"text\"}\n",
    "    if not required_cols.issubset(set(df.columns)):\n",
    "        raise SystemExit(f\"–û–∂–∏–¥–∞–ª–∏—Å—å –∫–æ–ª–æ–Ω–∫–∏: {required_cols}. –ï—Å—Ç—å: {set(df.columns)}\")\n",
    "\n",
    "    logger.info(f\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑\n",
    "    logger.info(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {args.model_name}...\")\n",
    "    model = SentenceTransformer(args.model_name, device='cuda')\n",
    "\n",
    "    rows_csv = []\n",
    "    rows_jsonl = []\n",
    "\n",
    "    web_ids = set()\n",
    "    seen_chunk_ids = set()\n",
    "\n",
    "    for _, r in tqdm(df.iterrows(), total=len(df), desc=\"–ß–∞–Ω–∫–∏–Ω–≥\"):\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∏\n",
    "        web_id_raw = r.get(\"web_id\")\n",
    "        if pd.isna(web_id_raw):\n",
    "            logger.warning(f\"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏: web_id –ø—É—Å—Ç–æ–π: {r.name}\")\n",
    "            continue\n",
    "        try:\n",
    "            web_id = int(web_id_raw)\n",
    "        except (ValueError, TypeError):\n",
    "            logger.warning(f\"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏: web_id –Ω–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –≤ int: {web_id_raw}\")\n",
    "            continue\n",
    "\n",
    "        title = r.get(\"title\")\n",
    "        url = r.get(\"url\")\n",
    "        kind = r.get(\"kind\")\n",
    "        raw_text = r.get(\"text\")\n",
    "\n",
    "        if pd.isna(title) or pd.isna(kind) or pd.isna(raw_text):\n",
    "            logger.warning(f\"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏: –æ–¥–∏–Ω –∏–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π –ø—É—Å—Ç: {r.name}\")\n",
    "            continue\n",
    "\n",
    "        title = str(title).strip()\n",
    "        url = str(url).strip() if pd.notna(url) else \"\"\n",
    "        kind = str(kind).strip()\n",
    "        raw_text = str(raw_text).strip()\n",
    "\n",
    "        if len(raw_text) < 100:\n",
    "            logger.info(f\"–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏: —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {r.name}\")\n",
    "            continue\n",
    "\n",
    "        # –û—á–∏—â–∞–µ–º –∏ —á–∞–Ω–∫—É–µ–º\n",
    "        clean_full_text = clean_text(raw_text)\n",
    "        chunks = semantic_chunk_by_similarity(\n",
    "            clean_full_text,\n",
    "            model,\n",
    "            max_chunk_len=args.max_chunk_len,\n",
    "            threshold=args.threshold\n",
    "        )\n",
    "\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunk_id = f\"{web_id}__{i}\"\n",
    "\n",
    "            if chunk_id in seen_chunk_ids:\n",
    "                logger.warning(f\"–ü—Ä–æ–ø—É—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–∞ chunk_id: {chunk_id}\")\n",
    "                continue\n",
    "            seen_chunk_ids.add(chunk_id)\n",
    "\n",
    "            # –¥–ª—è CSV: text ‚Äî —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –±–µ–∑ kind –∏ title\n",
    "            rows_csv.append({\n",
    "                \"web_id\": web_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"kind\": kind,\n",
    "                \"text\": chunk_text\n",
    "            })\n",
    "            # –¥–ª—è JSONL: —Ç–æ–∂–µ —Å–∞–º–æ–µ\n",
    "            rows_jsonl.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": chunk_text,\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"web_id\": web_id,\n",
    "                \"kind\": kind\n",
    "            })\n",
    "\n",
    "        web_ids.add(web_id)\n",
    "\n",
    "    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ text\n",
    "    seen_hashes = set()\n",
    "    unique_csv = []\n",
    "    unique_jsonl = []\n",
    "    for csv_row, jsonl_row in zip(rows_csv, rows_jsonl):\n",
    "        h = hash_text(csv_row[\"text\"])\n",
    "        if h in seen_hashes:\n",
    "            continue\n",
    "        seen_hashes.add(h)\n",
    "        unique_csv.append(csv_row)\n",
    "        unique_jsonl.append(jsonl_row)\n",
    "\n",
    "    logger.info(f\"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(rows_csv) - len(unique_csv)}\")\n",
    "\n",
    "    # --- –°–¢–ê–¢–ò–°–¢–ò–ö–ê ---\n",
    "    chunks_count = len(unique_csv)\n",
    "    total_len = sum(len(row[\"text\"]) for row in unique_csv)\n",
    "    avg_len = total_len / chunks_count if chunks_count > 0 else 0\n",
    "    uniq_web_ids = len(web_ids)\n",
    "\n",
    "    logger.info(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    logger.info(f\"   - –ß–∞–Ω–∫–æ–≤: {chunks_count}\")\n",
    "    logger.info(f\"   - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_len:.2f}\")\n",
    "    logger.info(f\"   - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö web_id: {uniq_web_ids}\")\n",
    "\n",
    "    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï CSV ---\n",
    "    out_df = pd.DataFrame(unique_csv, columns=[\"web_id\", \"chunk_id\", \"title\", \"url\", \"kind\", \"text\"])\n",
    "    out_df.to_csv(args.output, index=False)\n",
    "    logger.info(f\"chunks.csv: {len(out_df)} —á–∞–Ω–∫–æ–≤\")\n",
    "\n",
    "    # --- –î–û–ë–ê–í–õ–ï–ù–ò–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò –í –ö–û–ù–ï–¶ CSV ---\n",
    "    with open(args.output, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\\n#chunks_count,{chunks_count}\\n\")\n",
    "        f.write(f\"#avg_len,{avg_len:.2f}\\n\")\n",
    "        f.write(f\"#uniq_web_ids,{uniq_web_ids}\\n\")\n",
    "    logger.info(f\"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –∫–æ–Ω–µ—Ü {args.output}\")\n",
    "\n",
    "    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï JSONL ---\n",
    "    jsonl_path = args.output.replace(\".csv\", \".jsonl\")\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in unique_jsonl:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        # --- –î–û–ë–ê–í–õ–ï–ù–ò–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò –í –ö–û–ù–ï–¶ JSONL ---\n",
    "        stats = {\n",
    "            \"chunks_count\": chunks_count,\n",
    "            \"avg_len\": round(avg_len, 2),\n",
    "            \"uniq_web_ids\": uniq_web_ids\n",
    "        }\n",
    "        f.write(json.dumps(stats, ensure_ascii=False) + \"\\n\")\n",
    "    logger.info(f\"chunks.jsonl: {len(unique_jsonl)} —á–∞–Ω–∫–æ–≤ ‚Üí –¥–ª—è Generator\")\n",
    "    logger.info(f\"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –∫–æ–Ω–µ—Ü {jsonl_path}\")"
   ],
   "metadata": {
    "id": "e7HGZSScRV-k",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763040419908,
     "user_tz": -180,
     "elapsed": 17471,
     "user": {
      "displayName": "Alexander Belyanchikov",
      "userId": "09336036699414271733"
     }
    }
   },
   "id": "e7HGZSScRV-k",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# –Ø—á–µ–π–∫–∞: –ó–∞–ø—É—Å–∫ main() —Å GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU\")\n",
    "    device = 'cpu'\n",
    "\n",
    "class Args:\n",
    "    input = \"websites_updated.csv\"  # –∏–ª–∏ .xlsx\n",
    "    output = \"chunks.csv\"\n",
    "    model_name = \"all-MiniLM-L6-v2\"\n",
    "    max_chunk_len = 400\n",
    "    threshold = 0.5\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU\n",
    "model = SentenceTransformer(args.model_name, device=device)\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º main —Å –º–æ–¥–µ–ª—å—é\n",
    "main(args)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWm8uierRd_s",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763040583499,
     "user_tz": -180,
     "elapsed": 163586,
     "user": {
      "displayName": "Alexander Belyanchikov",
      "userId": "09336036699414271733"
     }
    },
    "outputId": "9c9c437a-f342-4756-dae4-641226e3ee42"
   },
   "id": "KWm8uierRd_s",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU: Tesla T4\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "–ß–∞–Ω–∫–∏–Ω–≥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 1935/1938 [02:37<00:00, 21.60it/s]WARNING:chunker:–ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏: –æ–¥–∏–Ω –∏–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π –ø—É—Å—Ç: 1937\n",
      "–ß–∞–Ω–∫–∏–Ω–≥: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1938/1938 [02:37<00:00, 12.30it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "df_result = pd.read_csv(\"chunks.csv\")\n",
    "print(df_result.head())\n",
    "print(df_result.shape)\n",
    "print()\n",
    "print(\"–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:\")\n",
    "print(df_result.iloc[0]['text'][:300] + \"...\")"
   ],
   "metadata": {
    "id": "mZ33DV6JRfnZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1763040583946,
     "user_tz": -180,
     "elapsed": 441,
     "user": {
      "displayName": "Alexander Belyanchikov",
      "userId": "09336036699414271733"
     }
    },
    "outputId": "0a538d6b-c020-44b0-d399-3f56daf4c189"
   },
   "id": "mZ33DV6JRfnZ",
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  web_id chunk_id                                              title  \\\n",
      "0      1     1__0  –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫ - –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∏ –¥–µ–±–µ—Ç–æ–≤—ã–µ –∫–∞—Ä—Ç—ã, –∫—Ä–µ–¥...   \n",
      "1      2     2__0                      –ê-–ö–ª—É–±. –î–µ–Ω—å–≥–∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–µ–Ω–∏–µ   \n",
      "2      2     2__1                      –ê-–ö–ª—É–±. –î–µ–Ω—å–≥–∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–µ–Ω–∏–µ   \n",
      "3      2     2__2                      –ê-–ö–ª—É–±. –î–µ–Ω—å–≥–∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–µ–Ω–∏–µ   \n",
      "4      2     2__3                      –ê-–ö–ª—É–±. –î–µ–Ω—å–≥–∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–µ–Ω–∏–µ   \n",
      "\n",
      "                           url  kind  \\\n",
      "0         https://alfabank.ru/  html   \n",
      "1  https://alfabank.ru/a-club/  html   \n",
      "2  https://alfabank.ru/a-club/  html   \n",
      "3  https://alfabank.ru/a-club/  html   \n",
      "4  https://alfabank.ru/a-club/  html   \n",
      "\n",
      "                                                text  \n",
      "0  –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –≤—ã —Å–º–æ–∂–µ—Ç–µ —É–∑–Ω–∞—Ç—å –ø–æ—Å–ª–µ –æ...  \n",
      "1  –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π: –±–∏—Ä–∂–µ–≤–∞—è, –≤–Ω–µ–±–∏—Ä–∂–µ...  \n",
      "2  –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥–∞—á–∏ –∫–∞–ø–∏—Ç–∞–ª–∞ –ê–ª—å—Ñ–∞‚Äë–ö–∞–ø–∏—Ç...  \n",
      "3  –ê–û ¬´–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫¬ª, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –ì–µ–Ω–µ—Ä–∞–ª—å–Ω–∞—è ...  \n",
      "4  –ê–û ¬´–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫¬ª –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∞–π–ª—ã ¬´cookie¬ª —Å —Ü–µ...  \n",
      "(23632, 6)\n",
      "\n",
      "–ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞:\n",
      "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –≤—ã —Å–º–æ–∂–µ—Ç–µ —É–∑–Ω–∞—Ç—å –ø–æ—Å–ª–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è –∑–∞—è–≤–∫–∏ –ü–æ–ª—É—á–∞–π—Ç–µ –±–æ–ª—å—à–µ —Å –ê–ª—å—Ñ–∞-–ë–∞–Ω–∫–æ–º –°–µ—Ä–≤–∏—Å—ã –ö—É—Ä—Å—ã –≤–∞–ª—é—Ç –í–∞–ª—é—Ç–∞ | –ü–æ–∫—É–ø–∫–∞ | –ü—Ä–æ–¥–∞–∂–∞ | |---|---|---| –î–æ–ª–ª–∞—Ä –°–®–ê (USD) | 77 | 80,4 | –ï–≤—Ä–æ (EUR) | 92 | 94,5 | –ö–∏—Ç–∞–π—Å–∫–∏–π —é–∞–Ω—å (CNY) | 11,15 | 12,15 | –ë–æ–ª—å—à–µ –≤–∞–ª—é—Ç –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—É—Ä—Å–∞—Ö –æ–±–º–µ–Ω–∞ –∏–Ω–æ—Å—Ç—Ä–∞...\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
