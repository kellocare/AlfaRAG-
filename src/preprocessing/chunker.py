#!/usr/bin/env python3
# src/preprocessing/chunker.py
"""
–ß–∞–Ω–∫–∏–Ω–≥ websites_updated.xlsx ‚Üí chunks.csv
–í—Ö–æ–¥: data/raw/websites_updated.xlsx
–í—ã—Ö–æ–¥: data/processed/chunks.csv

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è:
- –†–∞–±–æ—Ç–∞–µ—Ç —Å .xlsx (—á–µ—Ä–µ–∑ openpyxl)
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å: "kind: {kind}; title: {title}; text: {text}"
- –ì–ª—É–±–æ–∫–∞—è –æ—á–∏—Å—Ç–∫–∞: ?oirutpspid=, tel., footer-—Ñ—Ä–∞–∑—ã, JSON-—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
- –ß–∞–Ω–∫–∏–Ω–≥: 300 —Å–ª–æ–≤, overlap=60
- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —á–∞–Ω–∫–æ–≤ (<50 —Å–∏–º–≤.)
- –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ MD5(text)
- –ù–û–í–û–ï: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- –ù–û–í–û–ï: –º–µ—Ç—Ä–∏–∫–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ –∫–æ–Ω–µ—Ü CSV –∏ JSONL
"""

import argparse
import os
import re
import hashlib
import json
from tqdm import tqdm
import pandas as pd


# --- –ó–∞–≥—Ä—É–∑–∫–∞ NOISE_PATTERNS –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ) ---
def load_noise_patterns(config_path: str = "src/preprocessing/noise_patterns.json"):
    default_patterns = [
        r'\?oirutpspid=[^&\s]*',
        r'\?oirutpspsc=[^&\s]*',
        r'\?oirutpspjs=[^&\s]*',
        r'#\S*',
        r'tel\.', r'Tel\.', r'—Ç–µ–ª\.', r'–¢–µ–ª\.', r'—Ç–µ–ª:', r'–¢–µ–ª:',
        r'¬©\s*–ê–ª—å—Ñ–∞-–ë–∞–Ω–∫',
        r'–ü–æ–ª—å–∑—É—è—Å—å —Å–∞–π—Ç–æ–º',
        r'—Å–æ–≥–ª–∞—à–∞–µ—Ç–µ—Å—å —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏',
        r'–ö–æ–Ω—Ç–∞–∫—Ç—ã', r'–ü–æ–¥–¥–µ—Ä–∂–∫–∞', r'–ö–∞—Ä—Ç–∞ —Å–∞–π—Ç–∞',
        r'–ú–æ—Å–∫–≤–∞', r'–†–æ—Å—Å–∏—è',
        r'199\d‚Äì202\d',
        r'https?://[^\s]+\.(?:png|jpg|jpeg|gif|pdf)',
        r'\{[^{}]*\}', # JSON-—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    ]

    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            try:
                config = json.load(f)
                return config.get("noise_patterns", default_patterns)
            except Exception:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞ {config_path}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
                return default_patterns
    else:
        print(f"‚ö†Ô∏è –ö–æ–Ω—Ñ–∏–≥ {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
        return default_patterns


NOISE_PATTERNS = load_noise_patterns()


def clean_text(s: str) -> str:
    """–£–¥–∞–ª–µ–Ω–∏–µ HTML –∏ —à—É–º–∞"""
    if not isinstance(s, str):
        return ""
    for pattern in NOISE_PATTERNS:
        s = re.sub(pattern, '', s, flags=re.IGNORECASE)
    s = re.sub(r'<script.*?</script>', ' ', s, flags=re.IGNORECASE | re.DOTALL)
    s = re.sub(r'<style.*?</style>', ' ', s, flags=re.IGNORECASE | re.DOTALL)
    s = re.sub(r'<[^>]+>', ' ', s)
    s = re.sub(r'\s+', ' ', s)
    s = re.sub(r'([.,;!?])\1+', r'\1', s)
    return s.strip()

def build_contextual_content(kind: str, title: str, text: str) -> str:
    """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    kind = clean_text(kind)
    title = clean_text(title)
    text = clean_text(text)
    return f"kind: {kind}; title: {title}; text: {text}"

def chunk_text_by_words(text: str, words_per_chunk: int = 300, overlap: int = 60) -> list[str]:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Å–ª–æ–≤–∞–º"""
    if not text or len(text.strip()) < 20:
        return []
    words = text.split()
    if len(words) <= words_per_chunk:
        return [text] if len(text) >= 50 else []
    step = max(1, words_per_chunk - overlap)
    chunks = []
    for start in range(0, len(words), step):
        end = start + words_per_chunk
        chunk = " ".join(words[start:end]).strip()
        if len(chunk) >= 50:
            chunks.append(chunk)
        if end >= len(words):
            break
    return chunks

def hash_text(text: str) -> str:
    """–•—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
    return hashlib.md5(text.encode('utf-8', errors='ignore')).hexdigest()

def main(args):
    os.makedirs(os.path.dirname(args.output), exist_ok=True)

    print(f"–ß—Ç–µ–Ω–∏–µ {args.input}...")
    df = pd.read_excel(args.input, engine="openpyxl")  # .xlsx
    required_cols = {"web_id", "url", "kind", "title", "text"}
    if not required_cols.issubset(set(df.columns)):
        raise SystemExit(f"–û–∂–∏–¥–∞–ª–∏—Å—å –∫–æ–ª–æ–Ω–∫–∏: {required_cols}. –ï—Å—Ç—å: {set(df.columns)}")

    # –ù–û–í–û–ï: —É–±–∏—Ä–∞–µ–º dropna, –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Ü–∏–∫–ª–µ
    print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(df)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    rows_csv = []
    rows_jsonl = []

    web_ids = set()

    for _, r in tqdm(df.iterrows(), total=len(df), desc="–ß–∞–Ω–∫–∏–Ω–≥"):
        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        web_id_raw = r.get("web_id")
        if pd.isna(web_id_raw):
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏: web_id –ø—É—Å—Ç–æ–π: {r.name}")
            continue
        try:
            web_id = int(web_id_raw)
        except (ValueError, TypeError):
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏: web_id –Ω–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –≤ int: {web_id_raw}")
            continue

        title = r.get("title")
        url = r.get("url")
        kind = r.get("kind")
        raw_text = r.get("text")

        if pd.isna(title) or pd.isna(kind) or pd.isna(raw_text):
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å—Ç—Ä–æ–∫–∏: –æ–¥–∏–Ω –∏–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π –ø—É—Å—Ç: {r.name}")
            continue

        title = str(title).strip()
        url = str(url).strip() if pd.notna(url) else ""
        kind = str(kind).strip()
        raw_text = str(raw_text).strip()

        full_content = build_contextual_content(kind, title, raw_text)
        chunks = chunk_text_by_words(full_content, words_per_chunk=args.words_per_chunk, overlap=args.overlap)

        for i, chunk_text in enumerate(chunks):
            chunk_id = f"{web_id}__{i}"
            # –¥–ª—è CSV
            rows_csv.append({
                "web_id": web_id,
                "chunk_id": chunk_id,
                "title": title,
                "url": url,
                "kind": kind,
                "text": chunk_text
            })
            # –¥–ª—è JSONL (—Ç–æ—á–Ω–æ —Ç–æ, —á—Ç–æ –∂–¥—ë—Ç Generator)
            rows_jsonl.append({
                "chunk_id": chunk_id,
                "text": chunk_text,
                "url": url,
                "title": title,
                "web_id": web_id,
                "kind": kind
            })

        web_ids.add(web_id)

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ text
    seen_hashes = set()
    unique_csv = []
    unique_jsonl = []
    for csv_row, jsonl_row in zip(rows_csv, rows_jsonl):
        h = hash_text(csv_row["text"])
        if h in seen_hashes:
            continue
        seen_hashes.add(h)
        unique_csv.append(csv_row)
        unique_jsonl.append(jsonl_row)

    print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(rows_csv) - len(unique_csv)}")

    # --- –°–¢–ê–¢–ò–°–¢–ò–ö–ê ---
    chunks_count = len(unique_csv)
    total_len = sum(len(row["text"]) for row in unique_csv)
    avg_len = total_len / chunks_count if chunks_count > 0 else 0
    uniq_web_ids = len(web_ids)

    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   - –ß–∞–Ω–∫–æ–≤: {chunks_count}")
    print(f"   - –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_len:.2f}")
    print(f"   - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö web_id: {uniq_web_ids}")

    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï CSV ---
    out_df = pd.DataFrame(unique_csv, columns=["web_id", "chunk_id", "title", "url", "kind", "text"])
    out_df.to_csv(args.output, index=False)
    print(f"chunks.csv: {len(out_df)} —á–∞–Ω–∫–æ–≤")

    # --- –î–û–ë–ê–í–õ–ï–ù–ò–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò –í –ö–û–ù–ï–¶ CSV ---
    with open(args.output, "a", encoding="utf-8") as f:
        f.write(f"\n#chunks_count,{chunks_count}\n")
        f.write(f"#avg_len,{avg_len:.2f}\n")
        f.write(f"#uniq_web_ids,{uniq_web_ids}\n")
    print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –∫–æ–Ω–µ—Ü {args.output}")

    # --- –°–û–•–†–ê–ù–ï–ù–ò–ï JSONL ---
    jsonl_path = args.output.replace(".csv", ".jsonl")
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for item in unique_jsonl:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
        # --- –î–û–ë–ê–í–õ–ï–ù–ò–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò –í –ö–û–ù–ï–¶ JSONL ---
        stats = {
            "chunks_count": chunks_count,
            "avg_len": round(avg_len, 2),
            "uniq_web_ids": uniq_web_ids
        }
        f.write(json.dumps(stats, ensure_ascii=False) + "\n")
    print(f"chunks.jsonl: {len(unique_jsonl)} —á–∞–Ω–∫–æ–≤ ‚Üí –¥–ª—è Generator")
    print(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –∫–æ–Ω–µ—Ü {jsonl_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ß–∞–Ω–∫–∏–Ω–≥ websites_updated.xlsx ‚Üí chunks.csv + chunks.jsonl")
    parser.add_argument("--input", default="data/raw/websites_updated.xlsx")
    parser.add_argument("--output", default="data/processed/chunks.csv")
    parser.add_argument("--words_per_chunk", type=int, default=300)
    parser.add_argument("--overlap", type=int, default=60)
    args = parser.parse_args()
    main(args)